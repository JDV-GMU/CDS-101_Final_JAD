---
title: "CDS 101 â€“ Final Project Report"
author: "Group Name / Members"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    css: "gmu-cds101.css"
    toc: true
    toc_depth: 3
    number_sections: true
  pdf_document:
    toc: true
---

> Replace this template text with your own writing, keeping the headings and overall structure aligned with the **CDS 101 project rubric**.

# 1. Problem Definition

We are going to be investigating a variety of factors that may affect the price of a diamond. Diamonds are highly sought-after gems, used in all sorts of precious jewelry. With a dataset containing information on many aspects of diamonds, we will determine the particular effects of the clarity, carats, and cut quality of any diamond upon its price. We believe that the clarity and carat of a diamond will be strong predictors of pricing while the cut quality will be a weaker predictor.

# 2. Data Acquisition & Description

The dataset is, quite simply, called "diamonds". It was obtained from the website Kaggle at the link https://www.kaggle.com/datasets/ayeshaseherr/diamonds. 

There are ten variables creating each column; three are categorical, these being the cut quality, color grade, and clarity rating. The remaining seven are carat, depth, table, x, y, z, and price. 

The variables x, y, and z are the length, width, and depth in millimeters, carats are a measure of diamond weight, table is the top width percentage of each diamond, and price is standardly measured in USD.

There are just under 54000 entries/rows in the dataset, making it very large overall. 

```{r setup-data, echo=FALSE, message=FALSE, warning=FALSE}
library(readr)
diamond_df <- read_csv("data/diamonds.csv")
```

# 3. Data Cleaning & Preprocessing

Describe the steps you took to make the data usable, for example:

- Handling missing values (drop, impute, etc.).  
- Fixing types (dates, numeric vs. categorical).  
- Removing outliers (if done).  
- Creating new features (feature engineering).  
- Any scaling or encoding.

```{r cleaning, eval=FALSE}
# Example pseudo-code for cleaning:
# data_clean <- data |>
#   dplyr::filter(!is.na(target_variable)) |>
#   dplyr::mutate(
#     new_feature = some_transformation(old_feature)
#   )
```

# 4. Exploratory Data Analysis (EDA)

This section maps directly to the **EDA** part of the rubric:

- Summary statistics  
- Visualizations  
- Interpretation connected to the research question  

```{r eda-summary, message=FALSE, warning=FALSE}
# library(dplyr)
# summary(data_clean)
```

```{r eda-plot-example, message=FALSE, warning=FALSE, fig.cap="Example histogram of a numeric variable (EDA rubric)."}
# library(ggplot2)
# ggplot(data_clean, aes(x = some_numeric_variable)) +
#   geom_histogram(binwidth = 5, color = "white") +
#   labs(x = "Some Variable", y = "Count", title = "Distribution of Some Variable")
```

# 5. Visualization Quality and Storytelling

Use this section to satisfy the **Visualization Quality** rubric criterion:

- Explain why your plot types are appropriate.  
- Comment on labels, legends, colors, and overall readability.  
- Mention any steps you took to make plots accessible and interpretable.

# 6. Modeling Approach

Explain how you framed the problem and which models you chose:

- Type of task (regression, classification, etc.).  
- Baseline model or heuristic, if used.  
- Main model(s) chosen and why they are appropriate.

# 7. Model Implementation & Evaluation

This corresponds to the **Model Implementation & Evaluation** rubric criterion.

Describe:

- Features used.  
- Data splitting strategy (train/test or cross-validation).  
- Metrics used (accuracy, F1, RMSE, etc.).  
- Tables/plots summarizing performance.  
- A short interpretation of each metric/plot.

```{r modeling, eval=FALSE}
# Example structure:
# set.seed(123)
# train_index <- sample(seq_len(nrow(data_clean)), size = 0.8 * nrow(data_clean))
# train <- data_clean[train_index, ]
# test  <- data_clean[-train_index, ]
#
# model <- glm(target ~ ., data = train, family = binomial)
# preds <- predict(model, newdata = test, type = "response")
# # compute metrics...
```

# 8. Conclusions & Recommendations

Summarize the key takeaways:

- Answer your original research question(s) directly.  
- Highlight the most important patterns or relationships you found.  
- Discuss limitations (data size, bias, missing variables, etc.).  
- Suggest possible extensions or future work.

# 9. Code Quality & Reproducibility

Briefly document how someone else can reproduce your results:

- Which R scripts or Rmd files to run.  
- Any required R packages.  

```{r session-info, echo=FALSE}
sessionInfo()
```

# 10. References

List any references you used, such as:

- Dataset documentation.  
- Research papers or articles.  
- Tutorials or blog posts.

# Appendix (Optional)

Include extra plots, diagnostic checks, or model comparisons if needed.
